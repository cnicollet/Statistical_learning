---
title: "Statistical_Learning_chap1"
output: html_document
---

1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.
  a - The sample size n is extremely large, and the number of predictors p is small.
    - Flexible statistical learning would perform better because the sample size is large enough to fit more parameters especially when p is small so it limits the variance.
    
  b - The number of predictors p is extremely large, and the number of observations n is small.
    - Less flexible statistical learning would perform better because flexible models are prone to overfit with p very large.
    
  c - The relationship between the predictors and response is highly non-linear.
    - Flexible statistical learning would perform better, because it is very flexible regarding the shape of the fit.
    
  d - The variance of the error terms, i.e. ??2 = Var(), is extremely high.
    - Flexible statistical learning would perform worse, because it will be likely to overfit.

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

  a - We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.
  
  - Regression problem for inference, with n = 500 and p = 3. 
    * profit
    * number of employees
    * industry
  
   b - We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.
   
    - Classification problem to predict product success or failure with n = 20 and p = 13
      * price 
      * marketing budget
      * competition price
      and 10 more

  c - We are interest in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For
each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

  - Regression problem to predict % change in the USD/Euro exchange rate whith n = 52 weeks and p = 3.
    * % change in the US market
    * change in the British market
    * % change in the German market


3. We now revisit the bias-variance decomposition.
  `variance` increases as flexibilty increases, because any change in the data will completely change the fit (highly sensitive to individual values).
  `bias` decreases as flexibility increases, because fewer assumptions are made about the shape of the fit.
  `train error` decreases as flexibility increases, because the model will perfectly fit the data and generate low MSE for the training data.
  `test error` will have a U shaped curve as it will depend on the bias-variance trade-off.
  `bayes error or irreducible error will always be the same.
  
5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?
  - advantages:
    - fits better data compared to less flexible approach with fewer prior assumptions
  - disadvantages:
    - prone to overfit
    - hard to interpret
  If data is complex or we are just interested in predictions, we may choose flexible approach

6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

  - Parametric statistical learning:
    * We make assumption about the shape of the data and choose a model that fits best the data.
    * The advantage is that we don't have too many parameters that need to be fit and we can add any prior assumptions. But is the assumptions or knwledge are wrong then the model will be too.
    
  - Non parametric statistical learning:
    * We don't need to make any assumptions regading the shape of the data, so can fit a wider range of shapes for the underlying data. 
    * The disavantage is that they need a large number of obs to finf the best fit.

#### Applied

```{r setup, include=FALSE}
library(ISLR)
library(MASS)
library(ggplot2)
data("College")
```


```{r echo=FALSE}
rownames(College) <- College[, 1]
College <- College[, -1]
summary(College)
```


```{r echo=FALSE}
pairs(College[, 1:10])
ggplot(College, aes(x = Private, y = Outstate)) + geom_boxplot()
```

```{r}
Elite <- rep('No', nrow(College))
Elite[College$Top10perc > 50]  = "Yes"
Elite <- as.factor(Elite)
College <- data.frame(College, Elite)
```

```{r}
summary(College)
ggplot(College, aes(y = Outstate, x = Elite)) + geom_boxplot()
hist(College$Outstate)
hist(College$Enroll)
hist(College$Expend)
hist(College$Apps)
```

```{r}
library(dplyr)
data("Auto")
auto <- na.omit(Auto)
glimpse(auto)
str(auto)
```

- 3 qualitative variables : cylinders, origin even if they are coded as numeric and name.
The remaining variables are quantitative.

```{r}
summary(auto)
sapply(auto[, 1:7], mean)
sapply(auto[, 1:7], sd)
```

cylinders, origin and name are qualitative variables, the remaining variables are quantitative.

The range of each quantitative variale is displayed above with summary function which shows the minimum and maximum. We can also use the range function for this purpose.

The mean and standard deviation are also shown in the summary result or displayed using sapply func.

d - Remove the 10th through 85th observations. Compute the range, mean, and standard deviation of each predictor in the subset of the data that remains?

```{r}
auto <- auto[-(10:85), ]
dim(auto)
summary(auto)
```

```{r}
pairs(Auto)
```

* `mpg` is negatively correlated with `horsepower`, `displacement` and `weight`
* `mpg` is slightly increasing with `year
* `horsepower` is negatively correlated with `acceleration`
* `horsepower` is positively correlated with `displacement` and `weight`
* `acceleration` is negatively correlated with `displacement` 

f - The realationship between `mpg` and the other predictors

```{r}
summary(lm(mpg ~ ., data = Auto))
```

The plots and the summary statistics show that there is a relationship between `mpg` and the other variables. 


10 - Boston data set
```{r}
library(MASS)
data(Boston)
dim(Boston)
```

a - There are `506 rows` and `14 columns`.

b - pairwise sactterplots

```{r}
pairs(Boston)
```

The realtionship between `crim` and the other predicors is not linear.

c - Are any of the predictors asociated with per capita crime rate?

```{r}
corrplot::corrplot(cor(Boston), type = "upper", tl.pos = "td", diag = F)
```

```{r}
library(reshape2)
ggplot(melt(Boston, id="crim"), aes(x=value, y=crim)) +
  geom_point() +
  facet_wrap(~variable, scales="free")
```
`crim` has a peak when:
  * `indus is around 20`
  * `rad > 20`
  * `zn  and chas is around 0`
  * `tax is between 600 and 700`
  * `ptratio is around 0`
It seems that crim is negatively correlated with `medv` and `dis` and `black`

d - Do any of the suburbs of Boston appear to have particularly high crime rates ? 
```{r}
ggplot(Boston, aes(x = crim)) + geom_histogram()
ggplot(Boston, aes(x = tax)) + geom_histogram()
ggplot(Boston, aes(x = ptratio)) + geom_histogram()
```

It seems that there are outliers with crim and tax but no clear pattern with ptratio.

e - How many of the suburbs in this data set bound the Charles river?

```{r}
table(Boston$chas)
```

f - median pupil-teacher ratio among the tows?

```{r}
median(Boston$tax)
```

g - Which suburb of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r}
Boston %>%
  filter(medv == min(medv)) %>%
  sapply(quantile)
```
```{r}
Boston %>%
  filter(medv == min(medv)) %>%
  sapply(quantile)
```

- chas, dis, rm and zn are at their lowest percentile
- age and rad are at their maximum values.
- crim, indus, rm, dis
- lstat, black, ptratio, tax, nox, indus and crim are above their 75% percentile.

h -  In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling

```{r}
Boston %>%
  filter(rm > 7) %>%
  nrow()

Boston %>%
  filter(rm > 8) %>%
  summarise_all(mean)
```

